## 5. Results

This section reports results in two complementary regimes: a **balanced development snapshot** (to compare models fairly) and an **imbalanced deployment snapshot** (to reflect real surveillance conditions on AAPL L2). We then summarize model components (supervised, hybrid, unsupervised) and the final ensemble with operational implications.

### 5.1 Datasets & Regimes

- **Balanced development snapshot** (model selection & tuning): class balance enforced; threshold chosen on validation.
- **Imbalanced deployment snapshot** (AAPL L2, realistic): 148,340 observations → 146,857 normal (**99.0%**), 1,483 anomalous (**1.0%**). After `scale_pos_weight` and threshold tuning, the classifier attains **high recall** with a **manageable false‑positive rate** for surveillance.

\::: {.callout-tip}
**Operational translation.** A 1.3% false‑positive rate on the majority class corresponds to roughly **18–20 alerts/day** for a single highly‑active name—typically acceptable with triage tooling. (See Confusion Matrix below.)
\:::

### 5.2 Supervised Classifier (XGBoost; RF as baseline)

**Balanced development snapshot (decision threshold τ ≈ 0.52):**

- **Accuracy:** 79.75%
- **Macro‑F1:** 0.795
- **AUC:** ≈ 0.95
- **Pos (manip‑like):** P=0.743, R=0.909, F1=0.818
- **Neg (regular):** P=0.883, R=0.686, F1=0.772

![Supervised metrics & curves.](images/roc_auc_rf_regression.png){#fig-results-supervised fig-cap="Balanced development snapshot: confusion, ROC/PR, and summary metrics" width=90%}

**Imbalanced deployment snapshot (AAPL L2):**

- **Overall accuracy:** \~0.99 (dominated by normal class; interpret with care)
- **Normal (Class 0):** P=1.00, R=0.99, F1=0.99
- **Anomaly (Class 1):** P=0.41, R=0.89, F1=0.56
- **Macro‑F1:** \~0.78; **Weighted‑F1:** \~0.99

**Confusion matrix (AAPL L2):** correctly detects **1,318/1,483** anomalies (**89% recall**), with **1,884** false positives from **146,857** normal observations (**1.3% FPR**).

![Confusion matrix — deployment snapshot.](rf_confusion_matrix.png){#fig-confusion fig-cap="AAPL L2 confusion matrix at the chosen operating point" width=70%}

### 5.3 Feature Importance & Interpretability

Top contributors emphasize microstructure dynamics rather than identity:
`spread (0.211) > roll_vol_10 (0.193) > dt_sec (0.067) > price_z (0.042) > price_delta (0.042)`; `symbol_MSFT (0.168)` appears but microstructure dominates.

![XGBoost feature importances.](images/feature_importance.png){#fig-importance fig-cap="Information‑gain importances for market microstructure features" width=75%}

### 5.4 Hybrid Sequence Anomaly Model (Bottlenecked Autoencoder → OC‑SVM)

Trained on L2 sequences (e.g., 25×features), compressed via a 128‑dim bottleneck, then scored by an OC‑SVM on embeddings. Optimized for surveillance‑friendly **recall** and **F4**:

- **Recall ≈ 0.84**, **F4 ≈ 0.81** on injected test subsequences.

![Hybrid anomaly results.](images/ae_confusion_matrix.png){#fig-hybrid-results fig-cap="Bottlenecked AE → OC‑SVM: validation/test summary" width=85%}

### 5.5 Unsupervised (DBSCAN)

DBSCAN surfaces **bursty, irregular, coordinated** quote activity without pre‑specifying cluster counts, while ignoring scattered background noise—well matched to attack epochs intermixed with calm.

![DBSCAN clusters vs. noise.](images/results_dbscan.png){#fig-dbscan-results fig-cap="DBSCAN labeling: dense burst clusters amid noise" width=80%}

### 5.6 Ensemble Performance & Calibration

We combine supervised probabilities with hybrid and DBSCAN flags using a blended score, then set an operating threshold via weekly subsamples (**20k points/week**):

```text
Inputs: p_xgb (XGBoost), flag_hybrid (AE→OC‑SVM), flag_dbscan
Blend:  score = w1 * p_xgb + w2 * 1[flag_hybrid] + w3 * 1[flag_dbscan]
Decision: alert if score ≥ Θ  (Θ tuned weekly by sampling)
```

![Ensemble overview & lift.](images/results_ensemble.png){#fig-ensemble-results fig-cap="Ensemble calibration and lift over single‑model baselines" width=90%}

### 5.7 Summary of Key Findings

- **High‑recall detection** is achievable in realistic, imbalanced settings with careful thresholding and class weighting.
- **Interpretable microstructure features** dominate importance, supporting regulatory explainability.
- **Sequence‑aware hybrid modeling** improves sensitivity to temporal attack structure.
- **Ensembling** stabilizes performance across liquidity regimes and reduces model‑specific blind spots.
- **Calendar effects** (e.g., **July 4** shortened week) materially impact counts/rates and are handled explicitly in evaluation and sampling.

\::: {.callout-note}
**At‑a‑glance metrics table.**

| Regime            |    Acc | Macro‑F1 |    AUC | Pos P | Pos R | Pos F1 | Neg P | Neg R | Neg F1 | Threshold |
| :---------------- | -----: | -------: | -----: | ----: | ----: | -----: | ----: | ----: | -----: | :-------: |
| Balanced (dev)    | 0.7975 |    0.795 | \~0.95 | 0.743 | 0.909 |  0.818 | 0.883 | 0.686 |  0.772 |   τ≈0.52  |
| Imbalanced (AAPL) | \~0.99 |   \~0.78 |      — |  0.41 |  0.89 |   0.56 |  1.00 |  0.99 |   0.99 |   tuned   |
| :::               |        |          |        |       |       |        |       |       |        |           |

> **Note on calendar effects:** Independence Day (July 4) closure impacts counts and rates in that week; our evaluation and sampling explicitly account for it.
