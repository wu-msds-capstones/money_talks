[
  {
    "objectID": "capstone.html",
    "href": "capstone.html",
    "title": "Identifying Quote Stuffing in Real-time Orderbooks",
    "section": "",
    "text": "The shift from traditional floor trading to electronic platforms has changed financial markets significantly. High-frequency trading (HFT) began in the early 2000s and now accounts for up to 70% of US equity trading volume. It has brought both efficiency gains and new types of market manipulation (Financial Innovation, 2015; PNAS, 2021). One of the most prevelant and troubling manipulative methods is quote stuffing, which is a strategy that involves traders quickly submitting and canceling large numbers of orders to create confusion in the market, slow down competitors’ systems, and gain trading advantages through fake information gaps.\nQuote stuffing is a complex method of market manipulation that takes advantage of the technology in today’s electronic markets. Unlike traditional manipulation techniques that relied on spreading false information or coordinated trading, quote stuffing works at the microsecond level within the market’s own data streams. Traders who use this tactic overwhelm order books with quotes they do not plan to execute, causing what regulators call “excessive messaging.” This can lower market quality and negatively impact other participants (SEC Division of Risk, Finance and Strategy).\nDetecting quote stuffing is difficult for regulators, exchanges, and market participants. Standard surveillance techniques often miss the subtle patterns of quick order submissions and cancellations that define this practice. Recent research using Thomson Reuters Tick History (TRTH) millisecond data from 2005-2016 highlights that today’s market complexity requires advanced analytical methods to tell the difference between legitimate high-frequency market making and manipulative quote stuffing (MDPI, 2022).\nHFT History HFT and Networked Markets HFT and Systematic Risk Domain of HFT What is Quote Stuffing NYSE Trading Rules\nSomething something, and then I state some citable facts (Zelner et al., 2022)."
  },
  {
    "objectID": "capstone.html#introduction",
    "href": "capstone.html#introduction",
    "title": "Identifying Quote Stuffing in Real-time Orderbooks",
    "section": "",
    "text": "The shift from traditional floor trading to electronic platforms has changed financial markets significantly. High-frequency trading (HFT) began in the early 2000s and now accounts for up to 70% of US equity trading volume. It has brought both efficiency gains and new types of market manipulation (Financial Innovation, 2015; PNAS, 2021). One of the most prevelant and troubling manipulative methods is quote stuffing, which is a strategy that involves traders quickly submitting and canceling large numbers of orders to create confusion in the market, slow down competitors’ systems, and gain trading advantages through fake information gaps.\nQuote stuffing is a complex method of market manipulation that takes advantage of the technology in today’s electronic markets. Unlike traditional manipulation techniques that relied on spreading false information or coordinated trading, quote stuffing works at the microsecond level within the market’s own data streams. Traders who use this tactic overwhelm order books with quotes they do not plan to execute, causing what regulators call “excessive messaging.” This can lower market quality and negatively impact other participants (SEC Division of Risk, Finance and Strategy).\nDetecting quote stuffing is difficult for regulators, exchanges, and market participants. Standard surveillance techniques often miss the subtle patterns of quick order submissions and cancellations that define this practice. Recent research using Thomson Reuters Tick History (TRTH) millisecond data from 2005-2016 highlights that today’s market complexity requires advanced analytical methods to tell the difference between legitimate high-frequency market making and manipulative quote stuffing (MDPI, 2022).\nHFT History HFT and Networked Markets HFT and Systematic Risk Domain of HFT What is Quote Stuffing NYSE Trading Rules\nSomething something, and then I state some citable facts (Zelner et al., 2022)."
  },
  {
    "objectID": "capstone.html#background",
    "href": "capstone.html#background",
    "title": "Identifying Quote Stuffing in Real-time Orderbooks",
    "section": "0.2 Background",
    "text": "0.2 Background\n\n0.2.1 The Evolution of Market Manipulation\nTraditional manipulation schemes relied on spreading false information or coordinating large trades to artificially influence prices. However, the digitization of financial markets has spawned a new generation of algorithmic manipulation techniques that operate at microsecond timescales, making them virtually impossible to detect through manual surveillance methods.\nSpoofing involves placing large orders with the intent to cancel them before execution, creating false impressions of supply or demand. The practice gained notoriety through cases like that of Navinder Singh Sarao, whose spoofing activities were linked to the 2010 Flash Crash. A single trader’s manipulation strategy contributed to a market event that erased nearly $1 trillion in market value within minutes, demonstrating the systemic risks posed by modern manipulation techniques.\nLayering extends this concept by placing multiple orders at different price levels to create an illusion of market depth. Manipulators build apparent support or resistance levels through phantom liquidity that disappears once other traders attempt to interact with it. This technique is particularly effective in less liquid securities where a few large orders can significantly influence price perception.\nQuote stuffing overwhelms market data systems with rapid order submissions and cancellations, creating information asymmetries that benefit high-frequency traders while disadvantaging other market participants. During peak manipulation periods, quote stuffing can generate thousands of order messages per second, effectively creating a denial-of-service attack against competing algorithms.\nMore sophisticated schemes include momentum ignition, where traders use rapid-fire orders to trigger algorithmic responses from other market participants, and pinging, which involves sending small orders to detect hidden liquidity in dark pools. These techniques exploit the fundamental asymmetry between human reaction times and algorithmic execution speeds, creating unfair advantages that violate market integrity principles.\n\n\n0.2.2 Regulatory Landscape and Enforcement Challenges\nThe regulatory response to algorithmic manipulation has been substantial but faces inherent technological limitations. The Securities and Exchange Commission (SEC) has pursued high-profile enforcement actions, including $920 million in penalties against major financial institutions since 2015. Notable cases include the $25 million fine against Deutsche Bank for spoofing activities and the $70 million penalty against JPMorgan Chase for manipulative trading in precious metals markets.\nThe European Union’s Markets in Financial Instruments Directive II (MiFID II) has implemented comprehensive transaction reporting requirements, mandating detailed records of order modifications, cancellations, and execution circumstances. Similarly, the Commodity Futures Trading Commission (CFTC) has established specific anti-spoofing regulations under the Dodd-Frank Act, creating criminal penalties for manipulation activities.\nDespite these regulatory efforts, traditional surveillance systems struggle with the volume and velocity of modern market data. Current approaches typically rely on post-trade analysis of aggregated data, processing daily volumes exceeding 50 billion order messages across U.S. equity markets. The latency between manipulation events and detection often spans hours or days, rendering enforcement reactive rather than preventive.\nRegulatory technology (RegTech) providers estimate that fewer than 5% of potential manipulation events are currently detected through automated surveillance, with most discoveries occurring through whistleblower reports or market anomaly investigations. This detection gap creates significant compliance risks for market operators and undermines confidence in market fairness.\n\n\n0.2.3 Technical Architecture Requirements\nEffective real-time manipulation detection requires processing Level 2 (L2) order book data, which provides granular visibility into bid and ask orders across multiple price levels. Unlike Level 1 data that shows only best bid/offer prices, L2 data reveals the full market depth, order queue dynamics, and the rapid changes in market microstructure that characterize manipulation attempts.\nThe technical challenges are substantial: processing millions of order book updates per second, maintaining sub-millisecond latency requirements, and distinguishing between legitimate high-frequency trading strategies and manipulative behavior. Modern exchanges generate L2 data feeds exceeding 2 terabytes daily for major securities, requiring specialized infrastructure for real-time analysis.\nMarket data complexity varies significantly across trading venues, with fragmented liquidity across over 16 registered exchanges and dozens of alternative trading systems in U.S. equity markets. Each venue maintains distinct order types, priority rules, and data formats, complicating unified surveillance efforts. Cross-market manipulation detection requires consolidating and normalizing data streams from multiple sources while maintaining temporal precision.\n\n\n0.2.4 Research Gap and Contribution\nWhile academic literature has explored manipulation detection using machine learning approaches, most studies rely on post-trade data analysis or simulated environments. Research by Cao et al. (2014) demonstrated machine learning applications for spoofing detection using historical futures data, while Goldstein et al. (2021) analyzed layering patterns in equity markets using daily aggregated data. However, real-time detection systems using live Level 2 market data remain largely proprietary to institutional market surveillance providers.\nThis proprietary nature creates a significant knowledge gap in the academic literature, limiting research reproducibility and innovation in detection methodologies. Commercial surveillance systems like NASDAQ’s SMARTS or Nice Actimize’s market surveillance solutions represent multi-million dollar investments that are inaccessible to academic researchers or smaller market participants.\nOur research addresses this gap by developing an open-source, real-time detection system that combines multiple data sources—live L2 order book feeds, historical tick data, and engineered microstructure features—to identify manipulation patterns as they occur. The system’s architecture enables both academic research and practical deployment by retail brokers or regulatory authorities.\nBy implementing synthetic manipulation injection capabilities, we can validate detection algorithms against known manipulation signatures while maintaining the ability to adapt to emerging schemes. This approach addresses the fundamental challenge of supervised learning in manipulation detection: the rarity of labeled manipulation events in historical data.\nThe system’s practical relevance extends beyond academic research. Retail brokers processing over $7 trillion in annual trading volume could implement similar detection capabilities to protect their clients from manipulation-induced losses. Regulatory authorities could enhance market surveillance through real-time monitoring rather than post-trade investigation, potentially preventing market disruptions before they occur.\nFurthermore, the transparency of our methodology enables market participants to understand surveillance capabilities, promoting fair competition through informed consent rather than information asymmetry. This represents a significant departure from the “black box” nature of current commercial surveillance systems."
  },
  {
    "objectID": "capstone.html#methods",
    "href": "capstone.html#methods",
    "title": "Identifying Quote Stuffing in Real-time Orderbooks",
    "section": "0.3 Methods",
    "text": "0.3 Methods\n\n0.3.1 Overview\nOur goal was to detect market manipulation behavior in high-frequency equity trading using publicly available data. We developed a machine learning model (XGBoost) to classify order book events as normal or anomalous, based on real market microstructure.\n\n\n0.3.2 1. Data Engineering\nWe combined three key data sources:\n\nLevel 2 Market Data (from Alpaca API): includes real-time bid/ask quotes, size, and book depth.\nSEC EDGAR Filings: matched stock filings (Forms 8-K, 10-K, 4) to time periods in market data to label known manipulative periods using enforcement actions.\nLabeled Anomalies: constructed labels for suspicious behavior using timing overlaps between L2 quote surges and known manipulation disclosures.\n\nAll data was stored and queried using PostgreSQL (via Beekeeper Studio), and preprocessing was handled in Python with pandas.\n\n\n\n0.3.3 2. Feature Engineering\nWe extracted the following features from quote data:\n\nprice: best bid or ask price at time ( t )\nsize: number of shares available at best price\ntotal_levels: depth of the order book (total number of visible price levels)\n\nThese were chosen for their interpretability and relevance to typical manipulation tactics like spoofing and layering.\nWe plan to expand the feature set in future iterations to include:\n\nOrder book imbalance\nQuote update frequency\nDerived volatility indicators\n\n\n\n\n0.3.4 3. Modeling Approach\nWe trained an XGBoost classifier, which is well-suited for imbalanced binary classification tasks and handles tabular data efficiently.\n\nTarget: Binary label (1 = anomaly, 0 = normal)\nTraining/test split: 80/20 stratified\nMetric: Log loss (eval_metric='logloss')\n\nTo address extreme class imbalance, we adjusted:\n\nscale_pos_weight = (neg / pos) to give more weight to anomalies\nExplored probability thresholds to tune precision/recall tradeoff\n\n\n\n\n0.3.5 4. Evaluation\nWe evaluated model performance using:\n\nClassification report (precision, recall, F1-score)\nConfusion matrix\nFeature importance plot (via xgboost.plot_importance)\n\nThreshold adjustments were explored using predict_proba to increase anomaly recall.\nWe implemented an XGBoost classifier (Chen2016?) to detect anomalies based on engineered features extracted from Level 2 quote data."
  },
  {
    "objectID": "capstone.html#the-data-we-needed",
    "href": "capstone.html#the-data-we-needed",
    "title": "Identifying Quote Stuffing in Real-time Orderbooks",
    "section": "1.1 The Data We Needed",
    "text": "1.1 The Data We Needed\n\n1.1.1 L2 Marketbook Data\n\nL1 Marketbook data, plus 5 layers above and below the L1 with regards to current bids and current asks\nAsks above yield upward price movement if filled, bids below yield downward prices if filled source\n\n\n\n1.1.2 L1 Marketbook Data\n\nThe equilibrium so to speak; the price at which the most recent order was placed and filled\nTimestamp down to microsecond, looks like: 2025-07-01 19:59:59.464006+00\nid, symbol, timestamp, bid_price, bid_size, ask_price, ask_size source\n\n\n\n1.1.3 OHLCV one-minute tick data\n\nOpen, High, Low, Close, Volume\nCollected with both minute and date timestamp\nAvailable up to 30 days in arears, including weekends even though there is no data fro weekends source"
  },
  {
    "objectID": "capstone.html#the-data-acquisition-process",
    "href": "capstone.html#the-data-acquisition-process",
    "title": "Identifying Quote Stuffing in Real-time Orderbooks",
    "section": "1.2 The Data Acquisition Process",
    "text": "1.2 The Data Acquisition Process\n As can be seen in the data flow above, the information needed to begin our project is generated in real-time as trades are placed on the New York Stock Exchange (NYSE) during trading hours (9:30 AM EDT until 3:30 PM EDT, though extended hours run from 9:00 AM EDT until 4:00 PM EDT) for Apple, Microsoft and Tesla stocks (tickers: AAPL; MSFT; TSLA). As a result, the origin of our data is the NYSE.\nTo get this capstone started on the right foot, our first piece of the data acquisition pipeline was made by establishing a postgres database and attaching it to a python script that utilized Alpaca-API to pick up historical OHLCV and L1 marketbook data needed for feature engineering. The OHLCV data spanned the one minute tick, giving us the symbol, date, timestamp, the stock’s opening and closing price, and the highs, lows and volume for the stock traded over that minute duration. Since the market is incredibly reactionary, this data is helpful in feature engineering and manipulation detection itself. The L1 marketbook data was acquired in a similar process to the OHLCV data, though it was streamed in real time and not the night after the market closes. With the free tier of an Alpaca account, the minute tick data was only available the following day, while the L1 data was able to be scraped on a 15-minute delay, so the API-calling process was the same for both data, but one was a stream and the other was pulled in a batch the night after market close.\nThe next piece of our byzantine data pipeline, and the most complex, was the L2 marketbook data. Given that we decided to focus on high-volume stocks, the millions of records that came with the live L2 marketbook data had to be streamed in real time. To do this, a gateway instance was established through Interactive Brokers (IBKR) on an AWS EC2 instance that streamed real-time data for each of the tickers. This gateway served as a medium between the exchange itself where the real-time data was posted, and our Postgres database where the data is stored. Once streamed through the gateway, the data was then parsed by the scraping script and sent two places: the first of which being the Postgres database established on Railway, and the second being the AWS S3 bucket in the form of a parquet file for that day’s worth of data; these parquet files serve as backups should anything happen to our Postgres database. Once this data stream was set up and established, perhaps the next most difficult part was checking for duplicates and filtering the data. This was extremely difficult do to the volume of the data and inconsistencies in the streaming updates. The gateway instance was designed to update only upon new updates, which were timestamped to the microsecond, but unfortunately sometimes it would add previously collected data with its new updates since the difference in timestamps was often so finite. To combat this, we had to check our dataset for duplicates, which we removed prior to the training of our models to the tune of roughly 128,000 duplicates over the three tickers. This may seem like a lot, but when facing just over 4 million rows, this came out to only 3% of our actual data.\nEC2 S3"
  },
  {
    "objectID": "capstone.html#the-data-workflow",
    "href": "capstone.html#the-data-workflow",
    "title": "Identifying Quote Stuffing in Real-time Orderbooks",
    "section": "1.3 The Data Workflow",
    "text": "1.3 The Data Workflow\nWith all of the data collection process enumerated above, the complete flow of the data–which can be found in the image above–looks like this: - L1 marketbook data Streamed directly into Postgres database from Alpaca-API; snapshot sent nightly as parquet file to S3 bucket - Snapshot created using cron jobs on Railway, data streamed through scraper code from github repo hosted on Railway - OHLCV data scraped nightly from Alpaca-API, deposited into Postgres database on Railway - Scraping done on cron job through Railway, given the data is available up to 30 days in arears through the API, snapshots were neither created nore stored - L2 marketbook data streamed in real-time from IBKR Gateway directly into Postgres database; snapshots captured nightly and stored in S3 bucket as parquet files - Gateway API accessed through scraping script on Railway from github repo, and the snapshots were done through a script that also executed the other snapshots\nCite something like this: (Smith2020?)."
  },
  {
    "objectID": "capstone.html#confusion-matrix",
    "href": "capstone.html#confusion-matrix",
    "title": "Identifying Quote Stuffing in Real-time Orderbooks",
    "section": "2.1 Confusion Matrix",
    "text": "2.1 Confusion Matrix\nThe confusion matrix below shows the model’s ability to detect anomalies in L2 quote data.\n\n\n\nConfusion Matrix"
  },
  {
    "objectID": "capstone.html#feature-importance",
    "href": "capstone.html#feature-importance",
    "title": "Identifying Quote Stuffing in Real-time Orderbooks",
    "section": "2.2 Feature Importance",
    "text": "2.2 Feature Importance\nFeature importance ranked by XGBoost’s internal gain metric:\n\n\n\nFeature Importance\n\n\n(Chen2016?)"
  }
]