<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Identifying Quote Stuffing in Real-time Orderbooks – Money Talks: The Unspoken World of Market Manipulation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-2fef5ea3f8957b3e4ecc936fc74692ca.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-a14e3238c51140e99ccc48519b6ed9ce.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-ccd81253fefb92e9cd3f9eb5b5d87b54.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-a14e3238c51140e99ccc48519b6ed9ce.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="site_libs/quarto-diagram/mermaid.css" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Money Talks: The Unspoken World of Market Manipulation</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="./about_us.qmd"> 
<span class="menu-text">About Us</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Identifying Quote Stuffing in Real-time Orderbooks</h1>
            <p class="subtitle lead">Observance of AAPL, MSFT, &amp; TSLA Data From NYSE</p>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Authors</div>
      <div class="quarto-title-meta-contents">
               <p>Cameron Hayman </p>
               <p>Ghson Alotibi </p>
               <p>Paxton Jones </p>
            </div>
    </div>
      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a>
  <ul class="collapse">
  <li><a href="#background" id="toc-background" class="nav-link" data-scroll-target="#background"><span class="header-section-number">1.1</span> Background</a></li>
  <li><a href="#common-market-manipulation-techniques" id="toc-common-market-manipulation-techniques" class="nav-link" data-scroll-target="#common-market-manipulation-techniques"><span class="header-section-number">1.2</span> Common Market Manipulation Techniques</a></li>
  <li><a href="#regulatory-landscape-and-enforcement-challenges" id="toc-regulatory-landscape-and-enforcement-challenges" class="nav-link" data-scroll-target="#regulatory-landscape-and-enforcement-challenges"><span class="header-section-number">1.3</span> Regulatory Landscape and Enforcement Challenges</a></li>
  <li><a href="#technical-architecture-requirements" id="toc-technical-architecture-requirements" class="nav-link" data-scroll-target="#technical-architecture-requirements"><span class="header-section-number">1.4</span> Technical Architecture Requirements</a></li>
  <li><a href="#research-gap-and-contribution" id="toc-research-gap-and-contribution" class="nav-link" data-scroll-target="#research-gap-and-contribution"><span class="header-section-number">1.5</span> Research Gap and Contribution</a></li>
  <li><a href="#methods" id="toc-methods" class="nav-link" data-scroll-target="#methods"><span class="header-section-number">1.6</span> Methods</a></li>
  <li><a href="#data-sources-and-collection" id="toc-data-sources-and-collection" class="nav-link" data-scroll-target="#data-sources-and-collection"><span class="header-section-number">1.7</span> Data Sources and Collection</a></li>
  <li><a href="#feature-engineering-and-data-processing" id="toc-feature-engineering-and-data-processing" class="nav-link" data-scroll-target="#feature-engineering-and-data-processing"><span class="header-section-number">1.8</span> Feature Engineering and Data Processing</a></li>
  <li><a href="#machine-learning-model-development" id="toc-machine-learning-model-development" class="nav-link" data-scroll-target="#machine-learning-model-development"><span class="header-section-number">1.9</span> Machine Learning Model Development</a></li>
  <li><a href="#detection-process" id="toc-detection-process" class="nav-link" data-scroll-target="#detection-process"><span class="header-section-number">1.10</span> Detection Process</a></li>
  <li><a href="#observation-level-scoring" id="toc-observation-level-scoring" class="nav-link" data-scroll-target="#observation-level-scoring"><span class="header-section-number">1.11</span> Observation-Level Scoring</a></li>
  <li><a href="#model-evaluation-and-validation" id="toc-model-evaluation-and-validation" class="nav-link" data-scroll-target="#model-evaluation-and-validation"><span class="header-section-number">1.12</span> Model Evaluation and Validation</a></li>
  </ul></li>
  <li><a href="#data" id="toc-data" class="nav-link" data-scroll-target="#data"><span class="header-section-number">2</span> Data</a>
  <ul class="collapse">
  <li><a href="#data-requirements-and-sources" id="toc-data-requirements-and-sources" class="nav-link" data-scroll-target="#data-requirements-and-sources"><span class="header-section-number">2.1</span> Data Requirements and Sources</a>
  <ul class="collapse">
  <li><a href="#level-2-l2-market-data" id="toc-level-2-l2-market-data" class="nav-link" data-scroll-target="#level-2-l2-market-data"><span class="header-section-number">2.1.1</span> Level 2 (L2) Market Data</a></li>
  <li><a href="#level-1-l1-market-data" id="toc-level-1-l1-market-data" class="nav-link" data-scroll-target="#level-1-l1-market-data"><span class="header-section-number">2.1.2</span> Level 1 (L1) Market Data</a></li>
  <li><a href="#ohlcv-minute-bar-data" id="toc-ohlcv-minute-bar-data" class="nav-link" data-scroll-target="#ohlcv-minute-bar-data"><span class="header-section-number">2.1.3</span> OHLCV Minute-Bar Data</a></li>
  </ul></li>
  <li><a href="#data-collection-infrastructure-and-process" id="toc-data-collection-infrastructure-and-process" class="nav-link" data-scroll-target="#data-collection-infrastructure-and-process"><span class="header-section-number">2.2</span> Data Collection Infrastructure and Process</a>
  <ul class="collapse">
  <li><a href="#target-securities-and-trading-sessions" id="toc-target-securities-and-trading-sessions" class="nav-link" data-scroll-target="#target-securities-and-trading-sessions"><span class="header-section-number">2.2.1</span> Target Securities and Trading Sessions</a></li>
  <li><a href="#historical-and-streaming-data-collection" id="toc-historical-and-streaming-data-collection" class="nav-link" data-scroll-target="#historical-and-streaming-data-collection"><span class="header-section-number">2.2.2</span> Historical and Streaming Data Collection</a></li>
  <li><a href="#level-2-data-streaming-architecture" id="toc-level-2-data-streaming-architecture" class="nav-link" data-scroll-target="#level-2-data-streaming-architecture"><span class="header-section-number">2.2.3</span> Level 2 Data Streaming Architecture</a></li>
  <li><a href="#data-quality-and-deduplication-challenges" id="toc-data-quality-and-deduplication-challenges" class="nav-link" data-scroll-target="#data-quality-and-deduplication-challenges"><span class="header-section-number">2.2.4</span> Data Quality and Deduplication Challenges</a></li>
  </ul></li>
  <li><a href="#integrated-data-workflow-and-orchestration" id="toc-integrated-data-workflow-and-orchestration" class="nav-link" data-scroll-target="#integrated-data-workflow-and-orchestration"><span class="header-section-number">2.3</span> Integrated Data Workflow and Orchestration</a></li>
  </ul></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results"><span class="header-section-number">3</span> Results</a>
  <ul class="collapse">
  <li><a href="#dataset-overview-and-model-performance" id="toc-dataset-overview-and-model-performance" class="nav-link" data-scroll-target="#dataset-overview-and-model-performance"><span class="header-section-number">3.1</span> Dataset Overview and Model Performance</a></li>
  <li><a href="#detailed-performance-analysis" id="toc-detailed-performance-analysis" class="nav-link" data-scroll-target="#detailed-performance-analysis"><span class="header-section-number">3.2</span> Detailed Performance Analysis</a></li>
  <li><a href="#confusion-matrix-analysis" id="toc-confusion-matrix-analysis" class="nav-link" data-scroll-target="#confusion-matrix-analysis"><span class="header-section-number">3.3</span> Confusion Matrix Analysis</a></li>
  <li><a href="#feature-importance-and-model-interpretability" id="toc-feature-importance-and-model-interpretability" class="nav-link" data-scroll-target="#feature-importance-and-model-interpretability"><span class="header-section-number">3.4</span> Feature Importance and Model Interpretability</a></li>
  </ul></li>
  <li><a href="#conclusions" id="toc-conclusions" class="nav-link" data-scroll-target="#conclusions"><span class="header-section-number">4</span> Conclusions</a>
  <ul class="collapse">
  <li><a href="#still-in-the-process-of-improving-models" id="toc-still-in-the-process-of-improving-models" class="nav-link" data-scroll-target="#still-in-the-process-of-improving-models"><span class="header-section-number">4.0.1</span> Still in the process of improving models!!</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">5</span> References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">






<section id="introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>The shift from traditional floor trading to electronic platforms has transformed financial markets significantly. High-frequency trading (HFT) began in the early 2000s and now accounts for up to 70% of US equity trading volume <span class="citation" data-cites="Financial_Innovation_2015 PNAS_2021">(<a href="#ref-Financial_Innovation_2015" role="doc-biblioref"><strong>Financial_Innovation_2015?</strong></a>; <a href="#ref-PNAS_2021" role="doc-biblioref"><strong>PNAS_2021?</strong></a>)</span>. While HFT has brought efficiency gains, it has also enabled new forms of market manipulation, with quote stuffing emerging as one of the most prevalent and concerning tactics.</p>
<p>Quote stuffing is a manipulative strategy where traders rapidly submit and cancel large numbers of orders to overwhelm market data systems and create artificial delays for competitors. Unlike traditional manipulation that relied on false information or coordinated trading, quote stuffing exploits the technological infrastructure of modern electronic markets. Traders flood order books with “quotes” (buy and sell orders) they never intend to execute, creating what regulators term “excessive messaging.” This strategy benefits the manipulator by slowing down competing algorithms and creating temporary information advantages through market confusion, effectively giving them millisecond-level trading advantages that can translate to significant profits <span class="citation" data-cites="SEC_Risk_Finance_Strategy">(<a href="#ref-SEC_Risk_Finance_Strategy" role="doc-biblioref"><strong>SEC_Risk_Finance_Strategy?</strong></a>)</span>.</p>
<p>The detection challenge stems from the subtle nature of these attacks - while quote stuffing does disrupt markets, it operates at microsecond timescales within normal market data flows, making it difficult to distinguish from legitimate high-frequency market making activities. The disruption is intentionally designed to appear as natural market volatility while creating systematic advantages for the manipulator. Recent research using Thomson Reuters Tick History (TRTH), a comprehensive database of millisecond-precision market data from 2005-2016, demonstrates that advanced analytical methods are required to differentiate between legitimate trading and manipulative quote stuffing <span class="citation" data-cites="MDPI_2022">(<a href="#ref-MDPI_2022" role="doc-biblioref"><strong>MDPI_2022?</strong></a>)</span>.</p>
<section id="background" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="background"><span class="header-section-number">1.1</span> Background</h2>
<p>The evolution from traditional floor trading to algorithmic markets has fundamentally changed how manipulation occurs, creating new challenges for detection and regulation. This section examines the key manipulation techniques in modern markets and the technical requirements for effective surveillance.</p>
</section>
<section id="common-market-manipulation-techniques" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="common-market-manipulation-techniques"><span class="header-section-number">1.2</span> Common Market Manipulation Techniques</h2>
<p>To understand the challenge of detecting quote stuffing, it is essential to first examine the broader landscape of algorithmic manipulation techniques. Unlike traditional schemes that relied on spreading rumors or coordinating large trades, modern manipulation exploits the electronic infrastructure and operates at microsecond timescales.</p>
<p><strong>Spoofing</strong> involves placing large orders with the intent to cancel them before execution, creating false impressions of market supply or demand. Traders place these deceptive orders to influence other market participants’ behavior, then cancel them once the desired price movement occurs <span class="citation" data-cites="Cao_2014">(<a href="#ref-Cao_2014" role="doc-biblioref"><strong>Cao_2014?</strong></a>)</span>. The practice gained widespread attention through high-profile cases such as Navinder Singh Sarao, whose spoofing activities contributed to the 2010 Flash Crash. This single trader’s manipulation strategy played a role in a market event that temporarily erased nearly $1 trillion in market capitalization within minutes - representing paper losses in market valuation rather than actual wealth destruction, though the systemic risks demonstrated by such events remain concerning <span class="citation" data-cites="CFTC_Flash_Crash_Report">(<a href="#ref-CFTC_Flash_Crash_Report" role="doc-biblioref"><strong>CFTC_Flash_Crash_Report?</strong></a>)</span>.</p>
<p><strong>Layering</strong> builds upon spoofing by placing multiple deceptive orders at different price levels to create an artificial impression of market depth. Manipulators construct apparent support or resistance levels through “phantom liquidity” - orders that appear real in the order book but vanish when other traders attempt to execute against them. This technique proves particularly effective in thinly traded securities (stocks with lower daily trading volumes and fewer active participants) where even moderate-sized orders can significantly influence price perception and trader behavior.</p>
<p><strong>Quote stuffing</strong> represents the focus of our research and involves overwhelming market data systems with rapid order submissions and cancellations. Unlike spoofing or layering that create false price signals, quote stuffing attacks the market’s information processing capacity itself. This creates information asymmetries where the manipulator’s trading systems, designed to handle the flood of messages they generate, maintain functionality while competitors’ systems slow down or fail. The manipulator gains trading advantages by having faster access to real market conditions while others struggle with delayed or incomplete data - similar to how a distributed denial-of-service (DoS) attack overwhelms a website’s servers, except targeting the financial market’s data infrastructure. For readers less familiar with technology, imagine if one trader could slow down all other traders’ access to market information while maintaining their own full-speed access - this creates an unfair advantage in markets where milliseconds determine profitability.</p>
<p>More sophisticated schemes include <strong>momentum ignition</strong>, where traders use rapid-fire orders to trigger algorithmic responses from other market participants, and <strong>pinging</strong>, which involves sending small orders to detect hidden liquidity in dark pools. These techniques exploit the fundamental asymmetry between human reaction times and algorithmic execution speeds, creating unfair advantages that violate market integrity principles.</p>
</section>
<section id="regulatory-landscape-and-enforcement-challenges" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="regulatory-landscape-and-enforcement-challenges"><span class="header-section-number">1.3</span> Regulatory Landscape and Enforcement Challenges</h2>
<p>The regulatory response to algorithmic manipulation has been substantial but faces inherent technological limitations. The Securities and Exchange Commission (SEC) has pursued high-profile enforcement actions, including over $920 million in penalties against major financial institutions since 2015 <span class="citation" data-cites="SEC_Enforcement_Statistics_2021">(<a href="#ref-SEC_Enforcement_Statistics_2021" role="doc-biblioref"><strong>SEC_Enforcement_Statistics_2021?</strong></a>)</span>. Notable cases include the $25 million fine against Deutsche Bank for spoofing activities and the $70 million penalty against JPMorgan Chase for manipulative trading in precious metals markets <span class="citation" data-cites="CFTC_Deutsche_Bank_2021 CFTC_JPMorgan_2020">(<a href="#ref-CFTC_Deutsche_Bank_2021" role="doc-biblioref"><strong>CFTC_Deutsche_Bank_2021?</strong></a>; <a href="#ref-CFTC_JPMorgan_2020" role="doc-biblioref"><strong>CFTC_JPMorgan_2020?</strong></a>)</span>.</p>
<p>The European Union’s Markets in Financial Instruments Directive II (MiFID II) has implemented comprehensive transaction reporting requirements, mandating detailed records of order modifications, cancellations, and execution circumstances. The Commodity Futures Trading Commission (CFTC) has taken a different approach, establishing specific anti-spoofing regulations under the Dodd-Frank Act that create criminal penalties for manipulation activities. While both approaches aim to deter manipulation, they represent distinct regulatory philosophies - detailed record-keeping versus punitive enforcement.</p>
<p>Despite these regulatory efforts, traditional surveillance systems struggle with the volume and velocity of modern market data. Current approaches typically rely on post-trade analysis of aggregated data, processing daily volumes exceeding 50 billion order messages across U.S. equity markets. The latency between manipulation events and detection often spans hours or days, rendering enforcement reactive rather than preventive.</p>
<p>Regulatory technology (RegTech) providers estimate that fewer than 5% of potential manipulation events are currently detected through automated surveillance, with most discoveries occurring through whistleblower reports or market anomaly investigations. This detection gap creates significant compliance risks for market operators and undermines confidence in market fairness.</p>
</section>
<section id="technical-architecture-requirements" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="technical-architecture-requirements"><span class="header-section-number">1.4</span> Technical Architecture Requirements</h2>
<p>Effective real-time manipulation detection requires processing Level 2 (L2) order book data, which provides detailed visibility into all available buy and sell orders across multiple price levels. For example, while Level 1 data might show only that Apple stock has a best bid (highest buy offer) of $150.00 and a best ask (lowest sell offer) of $150.05, Level 2 data reveals the complete market depth: perhaps 500 shares bid at $150.00, another 300 shares bid at $149.99, 200 shares bid at $149.98, and so forth, along with corresponding ask levels. This full market depth reveals order queue dynamics (how orders are prioritized and filled) and the rapid changes in market microstructure (the detailed mechanics of how orders interact) that characterize manipulation attempts <span class="citation" data-cites="Hasbrouck_2007">(<a href="#ref-Hasbrouck_2007" role="doc-biblioref"><strong>Hasbrouck_2007?</strong></a>)</span>.</p>
<p>The technical challenges are substantial: processing millions of order book updates per second, maintaining sub-millisecond latency requirements, and distinguishing between legitimate high-frequency trading strategies and manipulative behavior. Modern exchanges generate L2 data feeds exceeding 2 terabytes daily for major securities <span class="citation" data-cites="NYSE_Market_Data_2022">(<a href="#ref-NYSE_Market_Data_2022" role="doc-biblioref"><strong>NYSE_Market_Data_2022?</strong></a>)</span>, requiring specialized infrastructure for real-time analysis.</p>
<p>Market data complexity varies significantly across trading venues, with fragmented liquidity (trading activity split across multiple venues) across over 16 registered exchanges and dozens of alternative trading systems in U.S. equity markets. Each venue maintains distinct order types (different ways to specify how trades should execute), priority rules (how competing orders are ranked), and data formats, complicating unified surveillance efforts. Cross-market manipulation detection requires consolidating and normalizing data streams from multiple sources while maintaining temporal precision.</p>
</section>
<section id="research-gap-and-contribution" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="research-gap-and-contribution"><span class="header-section-number">1.5</span> Research Gap and Contribution</h2>
<p>While academic literature has explored manipulation detection using machine learning approaches, most studies rely on post-trade data analysis or simulated environments. Research by Cao et al.&nbsp;(2014) demonstrated machine learning applications for spoofing detection using historical futures data (past market data from commodity and financial futures contracts), while Goldstein et al.&nbsp;(2021) analyzed layering patterns in equity markets using daily aggregated data <span class="citation" data-cites="Cao_2014 Goldstein_2021">(<a href="#ref-Cao_2014" role="doc-biblioref"><strong>Cao_2014?</strong></a>; <a href="#ref-Goldstein_2021" role="doc-biblioref"><strong>Goldstein_2021?</strong></a>)</span>. However, real-time detection systems using live Level 2 market data remain largely proprietary to institutional market surveillance providers due to the significant technological and financial barriers involved in developing such systems.</p>
<p>This proprietary nature creates a significant knowledge gap in the academic literature, limiting research reproducibility and innovation in detection methodologies. Commercial surveillance systems like NASDAQ’s SMARTS or Nice Actimize’s market surveillance solutions represent multi-million dollar investments that are inaccessible to academic researchers or smaller market participants.</p>
<p>Our research addresses this gap by developing an open-source, real-time detection system that combines multiple data sources—live L2 order book feeds, historical tick data, and engineered microstructure features (statistical measures derived from raw market data that capture trading patterns)—to identify manipulation patterns as they occur. The system’s architecture enables both academic research and practical deployment by retail brokers or regulatory authorities.</p>
<p>By implementing synthetic manipulation injection capabilities (the ability to artificially introduce known manipulation patterns into live data for testing purposes), we can validate detection algorithms against known manipulation signatures (characteristic patterns that identify specific manipulation techniques) while maintaining the ability to adapt to emerging schemes. This approach addresses the fundamental challenge of supervised learning in manipulation detection: the rarity of labeled manipulation events in historical data.</p>
<p>The system’s practical relevance extends beyond academic research. Retail brokers processing over $7 trillion in annual trading volume <span class="citation" data-cites="FINRA_Trading_Volume_2022">(<a href="#ref-FINRA_Trading_Volume_2022" role="doc-biblioref"><strong>FINRA_Trading_Volume_2022?</strong></a>)</span> could implement similar detection capabilities to protect their clients from manipulation-induced losses. Regulatory authorities could enhance market surveillance through real-time monitoring rather than post-trade investigation, potentially preventing market disruptions before they occur.</p>
<p>Furthermore, the transparency of our methodology enables market participants to understand surveillance capabilities, promoting fair competition through informed participation rather than information asymmetry (situations where some traders have access to information or capabilities that others lack). This represents a significant departure from the “black box” nature of current commercial surveillance systems.</p>
</section>
<section id="methods" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="methods"><span class="header-section-number">1.6</span> Methods</h2>
<p>This section details our comprehensive approach to detecting market manipulation in real-time trading environments. We developed a machine learning-based detection system using XGBoost classification applied to Level 2 market data, with extensive feature engineering and synthetic anomaly generation to address the challenge of rare manipulation events in historical data.</p>
</section>
<section id="data-sources-and-collection" class="level2" data-number="1.7">
<h2 data-number="1.7" class="anchored" data-anchor-id="data-sources-and-collection"><span class="header-section-number">1.7</span> Data Sources and Collection</h2>
<p>Our research combined three complementary data sources to create a comprehensive dataset for manipulation detection:</p>
<p><strong>Level 2 Market Data (Primary Source):</strong> We collected real-time order book data through the Alpaca Markets API, which provides institutional-grade access to NYSE and NASDAQ Level 2 data feeds. This includes bid and ask quotes with associated sizes (number of shares available) and book depth (number of visible price levels in the order book). Level 2 data captures the complete order book state, showing not just the best available prices but multiple price levels and their associated order quantities. For example, at any given moment, we might observe Apple stock with bids of 1000 shares at $150.00, 500 shares at $149.99, and 300 shares at $149.98, along with corresponding ask levels. This granular data enables detection of subtle manipulation patterns that would be invisible in standard Level 1 data feeds.</p>
<p><strong>SEC EDGAR Regulatory Filings:</strong> We systematically collected regulatory filings from the Securities and Exchange Commission’s EDGAR database, specifically targeting Forms 8-K (current reports), 10-K (annual reports), and Form 4 (insider trading reports) that disclosed enforcement actions or manipulation investigations. These filings provided ground truth labels for known manipulation periods, allowing us to identify specific time windows when manipulation activities were confirmed by regulatory action.</p>
<p><strong>Synthetic Anomaly Labels:</strong> To address the fundamental challenge of data imbalance in manipulation detection - where normal trading vastly outnumbers manipulation events - we developed a labeling methodology based on statistical anomaly detection. We identified suspicious periods by detecting significant surges in Level 2 quote message rates that coincided temporally with regulatory disclosure periods, creating a dataset of probable manipulation events for supervised learning.</p>
<hr>
</section>
<section id="feature-engineering-and-data-processing" class="level2" data-number="1.8">
<h2 data-number="1.8" class="anchored" data-anchor-id="feature-engineering-and-data-processing"><span class="header-section-number">1.8</span> Feature Engineering and Data Processing</h2>
<p>Our feature engineering process transformed raw Level 2 market data into analytically meaningful variables designed to capture manipulation patterns. We focused on creating interpretable features that reflect the specific characteristics of quote stuffing and related manipulation techniques.</p>
<p><strong>Core Market Microstructure Features:</strong></p>
<ul>
<li><p><strong><code>price</code></strong>: Best bid or ask price at time <em>t</em>, representing the most competitive available price for buying or selling. This captures basic price movement patterns that may indicate manipulation-induced volatility.</p></li>
<li><p><strong><code>size</code></strong>: Number of shares available at the best price level. Manipulation often involves placing unusually large orders that are quickly cancelled, creating distinctive patterns in order size distributions.</p></li>
<li><p><strong><code>total_levels</code></strong>: Depth of the order book, measured as the total number of visible price levels containing orders. Quote stuffing typically increases apparent market depth through phantom liquidity, making this a key indicator.</p></li>
</ul>
<p><strong>Advanced Manipulation Indicators:</strong></p>
<ul>
<li><p><strong>Order book imbalance</strong>: The ratio of total bid quantity to total ask quantity across all visible levels, calculated as (total_bid_size - total_ask_size) / (total_bid_size + total_ask_size). Manipulation techniques like quotestuffing create artificial imbalances that can be detected through this metric.</p></li>
<li><p><strong>Order rate per 100ms</strong>: Messages per second rate for each security, capturing the “excessive messaging” characteristic of quote stuffing attacks. Normal market making typically generates steady message rates, while manipulation creates distinctive spikes.</p></li>
<li><p><strong>Spread</strong>: The spread is the difference between the last ask price and the last bid price in the time window. Quotestuffing typically creates artificial spread patterns.</p></li>
</ul>
<p>Our feature selection prioritized interpretability and direct relevance to known manipulation tactics, ensuring that model predictions could be explained to regulatory authorities and market participants. The dataset preparation process involved extensive data validation and cleaning procedures to handle the inherent messiness of real-time market data feeds.</p>
<hr>
</section>
<section id="machine-learning-model-development" class="level2" data-number="1.9">
<h2 data-number="1.9" class="anchored" data-anchor-id="machine-learning-model-development"><span class="header-section-number">1.9</span> Machine Learning Model Development</h2>
<p>We selected XGBoost (eXtreme Gradient Boosting) as our primary classification algorithm for several technical and practical reasons. XGBoost is an advanced machine learning technique that builds multiple decision trees sequentially, where each new tree learns from the mistakes of previous trees. This approach excels at identifying complex patterns in structured data like our market microstructure features, and performs particularly well with imbalanced datasets where one class (normal trading) vastly outnumbers another (manipulation events).</p>
<p><strong>Model Configuration:</strong> - <strong>Target Variable</strong>: Binary classification where <code>1</code> indicates detected anomaly/manipulation and <code>0</code> indicates normal market behavior - <strong>Training/Testing Split</strong>: 80% of data used for model training, 20% reserved for unbiased performance evaluation, with stratified sampling to maintain class balance proportions in both sets - <strong>Evaluation Metric</strong>: Log loss (logarithmic loss), which penalizes confident but incorrect predictions more heavily than uncertain predictions, making it ideal for scenarios where false positives and false negatives have different costs</p>
<p><strong>Addressing Data Imbalance:</strong> The fundamental challenge in manipulation detection lies in extreme class imbalance - normal trading events outnumber manipulation events by ratios of 100:1 or higher. This imbalance can cause standard machine learning algorithms to achieve high accuracy by simply predicting “normal” for all cases while completely failing to detect manipulation.</p>
<p>We addressed this through several techniques: - <strong><code>scale_pos_weight</code></strong>: Set to the ratio of negative to positive samples (normal/anomaly), instructing XGBoost to treat each anomaly sample as equivalent to multiple normal samples during training - <strong>Threshold optimization</strong>: Rather than using the default 0.5 probability threshold for classification, we systematically explored different thresholds to optimize the precision/recall tradeoff based on business requirements (whether false positives or false negatives are more costly)</p>
<p><strong>Hybrid Bottlenecked Autoencoder/OC-SVM</strong></p>
<p>The system implements a two-stage hybrid anomaly detection approach specifically designed for identifying quote stuffing in financial markets. The method combines a modified Transformer autoencoder with a One-Class Support Vector Machine (OC-SVM) to create a powerful fraud detection system that learns normal market behavior and flags deviations. This approach is based on the methodology described by Poutre et al.&nbsp;(2024), who demonstrated that such hybrid frameworks achieve state-of-the-art performance in detecting trade-based manipulations without requiring prior knowledge of manipulation patterns.</p>
<p>An autoencoder is a neural network designed to learn efficient representations of data by training it to reconstruct its input. The network consists of two main components:</p>
<ol type="1">
<li><strong>Encoder</strong>: Maps input data <span class="math inline">\(X\)</span> to a lower-dimensional representation <span class="math inline">\(Z\)</span></li>
<li><strong>Decoder</strong>: Reconstructs the original input from the representation Mathematically, this can be expressed as: <span class="math display">\[
\begin{align}
Z &amp;= f_{\text{encoder}}(X) \\
\hat{X} &amp;= f_{\text{decoder}}(Z) \\
\mathcal{L} &amp;= ||X - \hat{X}||^2 \quad \text{(reconstruction error)}
\end{align}
\]</span></li>
</ol>
<p><strong>The Bottleneck Modification</strong></p>
<p>The “bottlenecked” aspect refers to a critical architectural choice that forces the model to learn compressed, meaningful representations. As described by Poutre et al.&nbsp;(2024), this modification was inspired by sentence embedding techniques but adapted specifically for financial time series anomaly detection. It functions as follows:</p>
<ol type="1">
<li><strong>Standard Transformer Processing</strong>: Input sequences (25 time windows × 23 features) pass through 6 Transformer encoder layers with multi-head attention</li>
<li><strong>Bottleneck Compression</strong>: The Transformer output is flattened and projected through a linear layer to just 128 dimensions<br>
</li>
<li><strong>Reconstruction</strong>: The 128-dimensional representation is projected back to the original dimensionality</li>
</ol>
<p>This bottleneck forces the model to learn the most essential patterns in normal market behavior, discarding noise and irrelevant details.</p>
<p><strong>Stage 2: One-Class SVM on Learned Representations</strong></p>
<p>One-Class SVM is an unsupervised learning algorithm designed to identify outliers by learning the boundary of normal data. Unlike traditional SVMs that separate two classes, OC-SVM finds a hyperplane that separates normal data from the origin in a high-dimensional space.</p>
</section>
<section id="detection-process" class="level2" data-number="1.10">
<h2 data-number="1.10" class="anchored" data-anchor-id="detection-process"><span class="header-section-number">1.10</span> Detection Process</h2>
<p>When detecting anomalies in new data, the system follows this pipeline:</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TD
    A[Raw Market Data] --&gt; B[Feature Extraction]
    B --&gt; C[Sequence Creation]
    C --&gt; D[Transformer Encoder]
    D --&gt; E[OC-SVM Decision Function]
    E --&gt; F[Classification]
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>The final dissimilarity score combines information from both components:</p>
<ul>
<li><strong>Autoencoder</strong>: How well the data can be reconstructed (implicit in representations)</li>
<li><strong>OC-SVM</strong>: How far the representation is from the learned normal boundary</li>
</ul>
</section>
<section id="observation-level-scoring" class="level2" data-number="1.11">
<h2 data-number="1.11" class="anchored" data-anchor-id="observation-level-scoring"><span class="header-section-number">1.11</span> Observation-Level Scoring</h2>
<p>The system implements a sophisticated observation-level scoring mechanism that accounts for the overlapping nature of sequences, following the methodology described by Poutre et al.&nbsp;(2024). Since each market observation can be part of multiple 25-step sequences, the final anomaly score for an observation is calculated as follows:</p>
<p><span class="math inline">\(\text{dissimilarity}(x_t) = \frac{1}{|S_{x_t}|} \sum_{s \in S_{x_t}} \text{dissimilarity}(s)\)</span></p>
<p>where <span class="math inline">\(S_{x_t}\)</span> is the set of all sequences containing observation <span class="math inline">\(x_t\)</span>. This approach ensures that “each event in X is also given at least one sequential dissimilarity value” while properly handling the overlapping nature of the sliding window approach.</p>
<hr>
</section>
<section id="model-evaluation-and-validation" class="level2" data-number="1.12">
<h2 data-number="1.12" class="anchored" data-anchor-id="model-evaluation-and-validation"><span class="header-section-number">1.12</span> Model Evaluation and Validation</h2>
<p>Our evaluation framework employed multiple complementary metrics to assess model performance across different aspects of manipulation detection effectiveness.</p>
<p><strong>Performance Metrics:</strong> - <strong>Classification Report</strong>: Provides precision (what percentage of predicted anomalies are actually anomalies), recall (what percentage of actual anomalies are correctly identified), and F1-score (harmonic mean of precision and recall) for both normal and anomaly classes. These metrics are crucial for understanding the tradeoffs between false positives (flagging normal activity as manipulation) and false negatives (missing actual manipulation).</p>
<ul>
<li><p><strong>Confusion Matrix</strong>: A 2x2 table showing the distribution of predictions versus actual labels, enabling clear visualization of model performance across both classes and identification of specific error patterns.</p></li>
<li><p><strong>Feature Importance Analysis</strong>: XGBoost’s built-in feature importance ranking based on information gain, showing which market microstructure features contribute most to manipulation detection. This analysis ensures model interpretability and helps identify the most relevant indicators for regulatory applications.</p></li>
</ul>
<p><strong>Validation Approach:</strong> We systematically explored probability threshold optimization using XGBoost’s <code>predict_proba</code> function, which returns prediction confidence scores rather than binary classifications. By adjusting the decision threshold above or below the default 0.5, we could tune the model to prioritize either precision (fewer false alarms) or recall (catching more manipulation), depending on the specific requirements of market surveillance applications.</p>
<p>The final implementation represents a balance between academic rigor and practical applicability, with extensive validation against both synthetic and real-world manipulation patterns <span class="citation" data-cites="Chen2016">(<a href="#ref-Chen2016" role="doc-biblioref"><strong>Chen2016?</strong></a>)</span>.</p>
</section>
</section>
<section id="data" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Data</h1>
<p>This section outlines the data acquisition strategy, sources, and processing pipeline for detecting quote stuffing manipulation in high-frequency trading environments. Our approach focuses on collecting multiple layers of market data to capture the microstructure patterns that characterize manipulative behavior.</p>
<section id="data-requirements-and-sources" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="data-requirements-and-sources"><span class="header-section-number">2.1</span> Data Requirements and Sources</h2>
<p>To effectively detect quote stuffing patterns, we required access to multiple levels of market data that provide comprehensive visibility into order book dynamics and trading activity. The following subsections detail each data type and its role in our analysis.</p>
<section id="level-2-l2-market-data" class="level3" data-number="2.1.1">
<h3 data-number="2.1.1" class="anchored" data-anchor-id="level-2-l2-market-data"><span class="header-section-number">2.1.1</span> Level 2 (L2) Market Data</h3>
<p>Level 2 market data extends beyond the basic bid-ask spread to provide a deeper view of market depth. This data includes the best bid and ask prices (Level 1 data) plus five additional price levels above and below the current market, along with the order quantities at each level <span class="citation" data-cites="InteractiveBrokers2024">(<a href="#ref-InteractiveBrokers2024" role="doc-biblioref"><strong>InteractiveBrokers2024?</strong></a>)</span>. This enhanced visibility is crucial for detecting quote stuffing because manipulative orders often appear at these secondary price levels rather than at the National Best Bid and Offer (NBBO). When orders at higher ask levels are filled, they typically drive prices upward, while orders at lower bid levels drive prices downward, creating the price volatility that quote stuffing strategies exploit.</p>
</section>
<section id="level-1-l1-market-data" class="level3" data-number="2.1.2">
<h3 data-number="2.1.2" class="anchored" data-anchor-id="level-1-l1-market-data"><span class="header-section-number">2.1.2</span> Level 1 (L1) Market Data</h3>
<p>Level 1 market data represents the current equilibrium point where the most recent transactions occurred. This data stream includes microsecond-precision timestamps (formatted as 2025-07-01 19:59:59.464006+00), unique identifiers, stock symbols, and the fundamental bid-ask spread components: bid price, bid size, ask price, and ask size <span class="citation" data-cites="Alpaca2024">(<a href="#ref-Alpaca2024" role="doc-biblioref"><strong>Alpaca2024?</strong></a>)</span>. The high temporal resolution is essential for quote stuffing detection because manipulative strategies often involve rapid order placement and cancellation cycles that occur within milliseconds.</p>
</section>
<section id="ohlcv-minute-bar-data" class="level3" data-number="2.1.3">
<h3 data-number="2.1.3" class="anchored" data-anchor-id="ohlcv-minute-bar-data"><span class="header-section-number">2.1.3</span> OHLCV Minute-Bar Data</h3>
<p>One-minute aggregated data captures the Open, High, Low, Close prices and trading Volume for each minute of market activity. This data includes both minute-level and date-level timestamps and remains available for up to 30 days through the API, including non-trading days where no actual market data exists <span class="citation" data-cites="Alpaca2024">(<a href="#ref-Alpaca2024" role="doc-biblioref"><strong>Alpaca2024?</strong></a>)</span>. While quote stuffing occurs at much finer time scales, minute-bar data provides essential context for identifying broader market conditions and volatility patterns that may coincide with manipulative activity.</p>
</section>
</section>
<section id="data-collection-infrastructure-and-process" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="data-collection-infrastructure-and-process"><span class="header-section-number">2.2</span> Data Collection Infrastructure and Process</h2>
<p>Our data acquisition strategy required a sophisticated real-time streaming infrastructure capable of handling the massive volume and velocity of high-frequency trading data. The system needed to capture microsecond-level market events across multiple data streams while maintaining data integrity and providing robust backup mechanisms. This section details the technical architecture and processes used to collect, validate, and store the market data essential for quote stuffing detection.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Capstone_Data_Collection__Process.png" class="img-fluid figure-img"></p>
<figcaption>Data Pipeline</figcaption>
</figure>
</div>
<section id="target-securities-and-trading-sessions" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="target-securities-and-trading-sessions"><span class="header-section-number">2.2.1</span> Target Securities and Trading Sessions</h3>
<p>We focused our data collection on three high-volume securities: Apple (AAPL), Microsoft (MSFT), and Tesla (TSLA). These stocks were selected because their high trading volumes and liquidity make them frequent targets for quote stuffing attacks, as manipulative strategies are most effective in actively traded securities where order flow can be more easily disguised. Data collection occurred during regular NYSE trading hours (9:30 AM to 4:00 PM EDT), with extended hours coverage (9:00 AM to 4:00 PM EDT) when available through our data providers.</p>
</section>
<section id="historical-and-streaming-data-collection" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="historical-and-streaming-data-collection"><span class="header-section-number">2.2.2</span> Historical and Streaming Data Collection</h3>
<p>The foundation of our data pipeline began with establishing a PostgreSQL database connected to Python scripts utilizing the Alpaca API for collecting both historical and real-time market data. For OHLCV minute-bar data, we implemented a batch collection process that retrieved historical data each evening after market close, as this aggregated data becomes available with a one-day delay under Alpaca’s free tier pricing. Each record contains the symbol, date, timestamp, opening and closing prices, intraday high and low prices, and total volume traded during the one-minute interval.</p>
<p>Level 1 market data required a different approach due to its real-time nature and importance for manipulation detection. We implemented a streaming collection system that captured live bid-ask spreads with a 15-minute delay (the minimum latency available through Alpaca’s free tier). This stream provided continuous updates of bid price, bid size, ask price, ask size, and microsecond-precision timestamps essential for identifying rapid order placement and cancellation patterns characteristic of quote stuffing.</p>
</section>
<section id="level-2-data-streaming-architecture" class="level3" data-number="2.2.3">
<h3 data-number="2.2.3" class="anchored" data-anchor-id="level-2-data-streaming-architecture"><span class="header-section-number">2.2.3</span> Level 2 Data Streaming Architecture</h3>
<p>The most technically challenging component of our pipeline involved collecting Level 2 market depth data, which required real-time streaming of millions of records per trading session. We deployed an Interactive Brokers (IBKR) TWS (Trader Workstation) Gateway instance on an Amazon Web Services (AWS) Elastic Compute Cloud (EC2) instance <span class="citation" data-cites="AWSEc22024">(<a href="#ref-AWSEc22024" role="doc-biblioref"><strong>AWSEc22024?</strong></a>)</span>. This gateway served as an intermediary between the exchange data feeds and our storage infrastructure, providing authenticated access to real-time Level 2 market depth information.</p>
<p>The streaming architecture routed incoming data to two destinations simultaneously. Primary storage occurred in our PostgreSQL database hosted on Railway’s cloud platform, providing immediate access for real-time analysis and model training. Concurrently, the system generated daily backup files in Apache Parquet format, which were stored in an AWS Simple Storage Service (S3) bucket <span class="citation" data-cites="AWSs32024">(<a href="#ref-AWSs32024" role="doc-biblioref"><strong>AWSs32024?</strong></a>)</span>. This dual-storage approach ensured data persistence and provided efficient columnar storage for large-scale analytical queries.</p>
</section>
<section id="data-quality-and-deduplication-challenges" class="level3" data-number="2.2.4">
<h3 data-number="2.2.4" class="anchored" data-anchor-id="data-quality-and-deduplication-challenges"><span class="header-section-number">2.2.4</span> Data Quality and Deduplication Challenges</h3>
<p>One of the most significant technical challenges involved managing data quality and eliminating duplicates in the high-velocity Level 2 stream. The IBKR gateway was configured to push updates only when new market events occurred, using microsecond timestamps to differentiate between events. However, the system occasionally transmitted duplicate records when timestamp differences were extremely small (often less than a microsecond apart) or when network latency caused delivery confirmation issues.</p>
<p>To address this challenge, we implemented a comprehensive deduplication process that identified and removed duplicate records based on combined timestamp, symbol, and price-size vector matching. Across all three target securities, this process eliminated approximately 128,000 duplicate records from a total dataset of over 4 million observations, representing roughly 3% of the collected data. While this may appear significant in absolute terms, the percentage represents an acceptable data quality threshold for high-frequency market data collection.</p>
</section>
</section>
<section id="integrated-data-workflow-and-orchestration" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="integrated-data-workflow-and-orchestration"><span class="header-section-number">2.3</span> Integrated Data Workflow and Orchestration</h2>
<p>The complete data workflow integrates multiple data streams through a coordinated pipeline that ensures comprehensive market coverage while maintaining data consistency and availability. Our orchestration system manages three distinct data collection processes, each optimized for its specific data type and latency requirements.</p>
<p>Level 1 market data flows directly from the Alpaca API into our PostgreSQL database through a continuous streaming process. The system captures real-time bid-ask updates with 15-minute delays and automatically generates nightly snapshots in Parquet format for backup storage in our S3 bucket. This process runs continuously during market hours through automated scripts deployed on Railway’s cloud platform, with source code version control maintained through our GitHub repository.</p>
<p>OHLCV minute-bar data collection operates on a batch schedule, scraping historical data each evening after market close through the Alpaca API. Since this aggregated data remains available for up to 30 days through the API, we implemented a nightly cron job execution pattern on Railway without requiring S3 backup storage. This approach optimizes storage costs while ensuring data availability for feature engineering and model training.</p>
<p>Level 2 market depth data represents the most complex workflow component, streaming real-time data from the IBKR Gateway directly into our PostgreSQL database. The system simultaneously captures nightly snapshots and stores them as Parquet files in S3 for long-term archival and analytical access. Gateway API access occurs through dedicated scraping scripts hosted on Railway, with snapshot generation coordinated through a unified script that manages backup processes across all data streams.</p>
</section>
</section>
<section id="results" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Results</h1>
<p>This section presents the performance evaluation of our XGBoost-based manipulation detection system applied to Apple Inc.&nbsp;(AAPL) Level 2 market data. Our analysis focused on three key areas: overall classification performance, model interpretability through feature importance analysis, and practical implications for real-world deployment.</p>
<section id="dataset-overview-and-model-performance" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="dataset-overview-and-model-performance"><span class="header-section-number">3.1</span> Dataset Overview and Model Performance</h2>
<p>Our final dataset consisted of 148,340 Level 2 quote observations from Apple stock trading, with a significant class imbalance reflecting the real-world rarity of manipulation events. The dataset contained 146,857 normal trading observations (99.0%) and 1,483 labeled anomalous events (1.0%), creating a challenging but realistic detection scenario that mirrors actual market surveillance conditions.</p>
<p>After implementing the <code>scale_pos_weight</code> optimization to address class imbalance, our XGBoost classifier demonstrated strong performance in detecting rare manipulation events while maintaining low false positive rates. The model achieved an overall accuracy of 99%, though this metric can be misleading in highly imbalanced scenarios and must be interpreted alongside precision and recall measures.</p>
<p>The pipeline</p>
</section>
<section id="detailed-performance-analysis" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="detailed-performance-analysis"><span class="header-section-number">3.2</span> Detailed Performance Analysis</h2>
<p>Our model’s performance breakdown reveals important insights into the practical applicability of machine learning for manipulation detection:</p>
<p><strong>Normal Trading Detection (Class 0):</strong> The model achieved near-perfect performance for normal trading identification, with precision of 1.00 (100% of predicted normal events were actually normal), recall of 0.99 (99% of actual normal events were correctly identified), and an F1-score of 0.99. This high performance on the majority class indicates the model successfully learned to distinguish standard market-making and trading patterns.</p>
<p><strong>Anomaly Detection (Class 1):</strong> Performance on manipulation detection proved more challenging but practically meaningful. The model achieved precision of 0.41 (41% of predicted anomalies were actual manipulation events) and recall of 0.89 (89% of actual manipulation events were successfully detected), resulting in an F1-score of 0.56.</p>
<p>The high recall (89%) demonstrates the model’s ability to catch the vast majority of manipulation attempts, which is critical for market surveillance applications where missing manipulation events could result in significant market harm. The moderate precision (41%) indicates that approximately 6 out of 10 alerts would be false positives, requiring human investigation but representing a manageable workload for compliance teams.</p>
<p><strong>Aggregate Performance Metrics:</strong> The macro average F1-score of 0.78 reflects balanced performance across both classes, while the weighted average of 0.99 is dominated by the majority class. For manipulation detection applications, the macro average provides a more meaningful assessment of overall system effectiveness.</p>
</section>
<section id="confusion-matrix-analysis" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="confusion-matrix-analysis"><span class="header-section-number">3.3</span> Confusion Matrix Analysis</h2>
<p>Figure 1 displays the confusion matrix visualization, providing intuitive insight into model performance patterns. The matrix reveals that our model correctly identified 1,318 of 1,483 actual manipulation events (89% recall), while generating 1,884 false positive alerts from the 146,857 normal observations (1.3% false positive rate).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="model_predictions.png" class="img-fluid figure-img"></p>
<figcaption>Figure 1: Confusion Matrix for XGBoost Manipulation Detection Model</figcaption>
</figure>
</div>
<p>This false positive rate translates to approximately 18-20 false alerts per trading day for a single stock, which represents a manageable investigation workload for market surveillance teams while maintaining high detection sensitivity.</p>
</section>
<section id="feature-importance-and-model-interpretability" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="feature-importance-and-model-interpretability"><span class="header-section-number">3.4</span> Feature Importance and Model Interpretability</h2>
<p>The XGBoost feature importance analysis reveals which market microstructure characteristics most effectively distinguish manipulation from normal trading activity. Figure 2 presents the ranked feature importance based on information gain, providing crucial insights for both model validation and practical implementation.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="feature_importance.png" class="img-fluid figure-img"></p>
<figcaption>Figure 2: Feature Importance Rankings for Market Manipulation Detection</figcaption>
</figure>
</div>
<p>The feature importance analysis demonstrates that our selected market microstructure variables effectively capture manipulation patterns, with each feature contributing meaningfully to the detection process. This interpretability proves essential for regulatory applications, where surveillance systems must provide explainable rationale for flagging suspicious trading activity.</p>
<p>The balanced contribution across features suggests that manipulation detection benefits from a holistic approach incorporating multiple market microstructure indicators rather than relying on any single metric, supporting our multi-feature engineering strategy <span class="citation" data-cites="Chen2016">(<a href="#ref-Chen2016" role="doc-biblioref"><strong>Chen2016?</strong></a>)</span>.</p>
</section>
</section>
<section id="conclusions" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Conclusions</h1>
<section id="still-in-the-process-of-improving-models" class="level3" data-number="4.0.1">
<h3 data-number="4.0.1" class="anchored" data-anchor-id="still-in-the-process-of-improving-models"><span class="header-section-number">4.0.1</span> Still in the process of improving models!!</h3>
<p>Cite something like this: <span class="citation" data-cites="Smith2020">(<a href="#ref-Smith2020" role="doc-biblioref"><strong>Smith2020?</strong></a>)</span>.</p>
</section>
</section>
<section id="references" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> References</h1>
<div id="refs" role="list">

</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/wu-msds-capstones\.github\.io\/money_talks\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>